{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab session 5: Classification and clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The purpose of this lab session is to provide you with an opportunity to gain experience in **classification** and **clustering**, covered in week 6, using common Python libraries.\n",
    "\n",
    "- This lab is the second part of a **two-week assignment** that covers weeks 5 and 6, which is due on **Friday 12th November 10am**.\n",
    "- The assignment will account for 10% of your overall grade. Questions in this lab sheet will contribute to 5% of your overall grade; questions in the lab sheet for week 5 will cover for another 5% of your overall grade.\n",
    "- <font color = 'maroon'>The last section of this notebook includes the questions that are assessed towards your final grade.</font> \n",
    "\n",
    "This session starts with a tutorial that uses examples to introduce you to the practical knowledge that you will need for the corresponding assignment. We highly recommend that you read the following tutorials if you need a gentler introduction to the libraries that we use:\n",
    "- [Numpy quickstart tutorial](https://numpy.org/devdocs/user/quickstart.html)\n",
    "- [Numpy: basic broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html)\n",
    "- [Pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html)\n",
    "- [Matplotlib](https://matplotlib.org/tutorials/introductory/pyplot.html)\n",
    "- [Seaborn](https://seaborn.pydata.org/tutorial/relational.html)\n",
    "- [Scikit-learn](https://scikit-learn.org/stable/tutorial/basic/tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important notes about the assignment: \n",
    "\n",
    "- **PLAGIARISM** <ins>is an irreversible non-negotiable failure in the course</ins> (if in doubt of what constitutes plagiarism, ask!). \n",
    "- The total assessed coursework is worth 40% of your final grade.\n",
    "- There will be 9 lab sessions and 4 assignments.\n",
    "- One assignment will cover 2 consecutive lab sessions and will be worth 10 marks (percentages of your final grade).\n",
    "- The submission cut-off date will be 7 days after the deadline and penalties will be applied for late submissions in accordance with the School policy on late submissions.\n",
    "- You are asked to submit a **report** that should answer the questions specified in the last section of this notebook. The report should be in a **single PDF document** (so **NOT** *doc, docx, notebook* etc). This single PDF document will include your answers to both the week 5 and week 6 labs.\n",
    "- No other means of submission other than submitting your assignment through the appropriate QM+ link are acceptable at any time. Submissions sent via email will **not** be considered.\n",
    "- Please name your report as follows: Assignment2-StudentName-StudentNumber.pdf\n",
    "- Cases of **Extenuating Circumstances (ECs)** have to go through the proper procedure of the School in due time. Only cases approved by the School in due time can be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "In order to present functionalities for data classification and clustering in Python, we use the [MNIST handwritten digits dataset](http://yann.lecun.com/exdb/mnist/) as part of a working example.\n",
    "\n",
    "The dataset is stored in a file called ``mnist.pkl.gz``, which is compressed by [gzip](https://docs.python.org/3/library/gzip.html) and serialized by [pickle](https://docs.python.org/3/library/pickle.html). \n",
    "\n",
    "In the snippet below, ``X`` is a matrix (numpy array) where each row corresponds to an observation and each column corresponds to a feature. Each observation is the result of *flattening* a 28 x 28 grayscale image of a handwritten digit into a vector. The list ``y`` (numpy array) contains the class (a digit from 0 to 9) corresponding to each observation (row) in the matrix ``X``.\n",
    "\n",
    "For the sake of this tutorial, we select only the first 2000 observations (rows) from the original MNIST training dataset to expedite computations.\n",
    "\n",
    "It is possible to use the ``matplotlib`` function ``imshow`` to visualise any observation by reshaping it appropriately. You may change the index ``i`` to select an observation for visualisation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring the appearance of ``seaborn`` graphics in this notebook\n",
    "%config InlineBackend.figure_formats = set(['retina'])\n",
    "\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# Selecting the training data from the original dataset\n",
    "f = gzip.open('data/mnist.pkl.gz', 'rb')\n",
    "X, y = pickle.load(f, encoding='latin1')[0]\n",
    "f.close()\n",
    "\n",
    "# Subsampling\n",
    "sample_size = 2000\n",
    "X, y = X[:sample_size], y[:sample_size]\n",
    "\n",
    "i = 10\n",
    "plt.imshow(X[i].reshape(28, 28), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.show()\n",
    "print('Observation index: {0}. Class: {1}.'.format(i, y[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library ``sklearn`` implements all the learning algorithms that we covered during the lectures. Each algorithm is implemented by a class that provides a standard interface.\n",
    "\n",
    "The class ``KNeighborsClassifier`` implements a k-nearest neighbour classification algorithm. The number of neighbours is specified by the constructor parameter ``n_neighbors``.\n",
    "\n",
    "The method ``KNeighborsClassifier.fit`` is responsible for learning a classifier for a dataset represented by an observation matrix and a class array. This is precisely how our data is represented.\n",
    "\n",
    "The method ``KNeighborsClassifier.score`` computes the accuracy of a classifier on a specific dataset. This method must be called after ``KNeighborsClassifier.fit``.\n",
    "\n",
    "As we have seen, a one-nearest neighbour classifier always predicts the correct class for any observation that already exists in a dataset (as long as there are no equal observations with different classes), which is why we obtain 100% accuracy on the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X, y)\n",
    "print('Training dataset accuracy: {0}.'.format(knn.score(X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate the capacity of a k-nearest neighbour classifier to assign unseen observations to correct classes, it is necessary to split the original dataset into a training dataset and a test dataset.\n",
    "\n",
    "The ``sklearn`` function ``train_test_split`` can be used for this purpose. This function selects a fraction of the observations (``test_size``) to compose the test set. The parameter ``random_state`` allows reproducibility by fixing the seed used by this (pseudo)random selection procedure. \n",
    "\n",
    "The function ``train_test_split`` returns a training observation matrix, a test observation matrix, a training class array, and a test class array. From now on, all classifiers are trained without access to the test dataset.\n",
    "\n",
    "The method ``KNeighborsClassifier.fit`` can be used to fit a k-nearest neighbour classifier to the training dataset. Naturally, the accuracy of the one-nearest neighbour classifier on the training set remains 100%. The accuracy on the test set, however, is a better estimate of the performance of the classifier on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)\n",
    "print('Training dataset accuracy: {0}.'.format(knn.score(X_train, y_train)))\n",
    "print('Test dataset accuracy: {0}.'.format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a ``KNeighborsClassifier`` is fit, the method ``KNeighborsClassifier.predict`` can be used to predict the classes for a list of observations.\n",
    "\n",
    "For example, we may use this method to predict the class for every observation in the test dataset. This allows obtaining the indices of the observations that are classified incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test)\n",
    "mistakes = np.nonzero(y_pred != y_test)[0]\n",
    "\n",
    "print('Indices of misclassified observations:')\n",
    "print(mistakes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the ``matplotlib`` function ``imshow`` to visualise a misclassified observation. You may change the index for ``mistakes`` in order to select an observation for visualisation (requires running all cells above, because ``y_pred`` is modified later). Note how some misclassifications are understandable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = mistakes[1]\n",
    "plt.imshow(X_test[i].reshape(28, 28), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.show()\n",
    "print('Observation index: {0}. Class: {1}. Prediction: {2}.'.format(i, y_test[i], y_pred[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some datasets need to be preprocessed before a learning algorithm can be applied. The training dataset must be preprocessed without access to the test dataset, otherwise the resulting test accuracy after preprocessing is likely to overestimate the accuracy on unseen data.\n",
    "\n",
    "The function ``make_pipeline`` allows chaining a sequence of preprocessing operations (such as individually standardizing each feature, which is accomplished by the class ``StandardScaler``) before a learning algorithm is applied. This function returns a ``Pipeline`` object that can be used to fit, predict, and score, just as any other classification algorithm in ``sklearn``. A ``Pipeline`` preprocesses incoming data appropriately before making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "knn_pipe = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=1))\n",
    "\n",
    "knn_pipe.fit(X_train, y_train)\n",
    "print('Test dataset accuracy: {0}.'.format(knn_pipe.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the task of selecting the best possible number of neighbours k (a hyperparameter) for the k-nearest neighbour classification algorithm.\n",
    "\n",
    "As we have seen, a hyperparameter should not be chosen based on its performance on the test dataset. In that case, there would be no data left to enable a reliable estimate of how well this choice generalizes to unseen data. After all, this choice would have been based on the test dataset.\n",
    "\n",
    "The function ``cross_validate`` conducts K-fold cross validation using a specified training dataset. Recall that cross-validation involves splitting a training dataset into K folds of equal size. Each fold is used as a validation set (where accuracies are computed) when the remaining folds are used as an effective training set (where a classifier is learned). The parameter ``cv`` controls the number of folds K. The function returns a dictionary with statistics that include a list of accuracies that can be accessed by the keyword ``'test_score'``.\n",
    "\n",
    "After a hyperparameter is selected by cross-validation, it can be used to fit a classifier to the entire training dataset, which can then be evaluated in the test dataset.\n",
    "\n",
    "Once again, if any step leading to model selection is based on the resulting performance on the test dataset, the resulting test accuracy is likely to overestimate the accuracy of the model on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_5 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_15 = KNeighborsClassifier(n_neighbors=15)\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Note that the data was already implicitly shuffled by train_test_split\n",
    "result_5 = cross_validate(knn_5, X_train, y_train, cv=5)\n",
    "result_15 = cross_validate(knn_15, X_train, y_train, cv=5)\n",
    "\n",
    "print('Average accuracy across folds (k = 5): {0}.'.format(result_5['test_score'].mean()))\n",
    "print('Average accuracy across folds (k = 15): {0}.'.format(result_15['test_score'].mean()))\n",
    "      \n",
    "knn_5.fit(X_train, y_train)\n",
    "print('Test dataset accuracy (k = 5): {0}.'.format(knn_5.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class ``GridSearchCV`` offers a convenient way to choose hyperparameters based on cross-validation. Its constructor receives a classification algorithm object (such as a ``KNeighborsClassifier`` object) and a dictionary that maps hyperparameter names to lists of values that should be considered. Once the method ``GridSearchCV.fit`` is called, each possible combination of hyperparameters from each of the lists is evaluated using cross-validation. The best hyperparameter setting is then used to fit a classifier to the training dataset. After fitting, the ``GridSearchCV`` object can be used to predict and score just as any other classification algorithm in ``sklearn``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'n_neighbors': [1, 3, 5, 7, 9]}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn_cv = GridSearchCV(knn, parameters, cv=5)\n",
    "knn_cv.fit(X_train, y_train)\n",
    "\n",
    "print('Best hyperparameter setting: {0}.'.format(knn_cv.best_estimator_))\n",
    "print('Average accuracy across folds of best hyperparameter setting: {0}.'.format(knn_cv.best_score_))\n",
    "print('Test dataset accuracy of best hyperparameter setting: {0}.'.format(knn_cv.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is not the only classification performance metric available on ``sklearn``. For instance, it is possible to compute the precision and recall for each class by considering the remaining classes as beloging to a single (negative) class. This is accomplished by the function ``precision_recall_fscore_support``.\n",
    "\n",
    "The function ``confusion_matrix`` creates a confusion matrix given a class array and a prediction array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = knn_cv.predict(X_test)\n",
    "\n",
    "precision, recall, _, _ = precision_recall_fscore_support(y_test, y_pred)\n",
    "print('Precision for each class: {0}.'.format(precision))\n",
    "print('Recall for each class: {0}.\\n'.format(recall))\n",
    "\n",
    "df = pd.DataFrame.from_records(confusion_matrix(y_test, y_pred))\n",
    "print('Confusion matrix:')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, ``sklearn`` implements learning algorithms covered during the lectures:  [support vector machines](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html), [artificial neural networks](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html), [decision trees](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), and [random forest classifiers](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
    "\n",
    "The interface provided by each of these learning algorithms is analogous to the interface provided by the k-nearest neighbour classification algorithm that we have used so far.\n",
    "\n",
    "For instance, in order to train a random forest classifier composed of 100 decision trees, an object of the class ``RandomForestClassifier`` can be constructed with ``n_estimators`` set to 100. This object can be used to fit the training dataset and compute the accuracy on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "rfc.fit(X_train, y_train)\n",
    "print(rfc.score(X_test, y_test))\n",
    "print('Test dataset accuracy (random forest classifier): {0}.'.format(rfc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "The clustering algorithms implemented by ``sklearn`` have an interface that is similar to the interface for the classification algorithms.\n",
    "\n",
    "The class ``KMeans`` implements the k-means clustering algorithm. The number of clusters ``n_clusters`` is a parameter for the constructor of this class. \n",
    "\n",
    "The method ``KMeans.fit_predict`` is equivalent to a call to ``KMeans.fit`` followed by a call to ``KMeans.predict``, which is responsible for attributing each observation to a cluster. After the clustering is computed by a call to ``KMeans.fit``, the sum of squared errors of the clustering is available as a class variable named ``inertia_``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=0)\n",
    "y_pred = kmeans.fit_predict(X)\n",
    "\n",
    "print('Sum of squared errors (k = 10): {0}.'.format(kmeans.inertia_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compare the quality of clusterings of the same dataset for different numbers of clusters, it is possible to employ both sums of squared errors and silhouette coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "max_k = 15\n",
    "\n",
    "# Sum of squared errors for each k\n",
    "sses = []\n",
    "\n",
    "# Silhouette coefficient for each k\n",
    "silhouettes = []\n",
    "\n",
    "for k in range(2, max_k + 1):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    y_pred = kmeans.fit_predict(X)\n",
    "    \n",
    "    sses.append(kmeans.inertia_)\n",
    "    silhouettes.append(silhouette_score(X, y_pred))\n",
    "\n",
    "# Plotting sums of squared errors\n",
    "df = pd.DataFrame({'sum of squared errors': sses, 'number of clusters': list(range(2, max_k + 1))})\n",
    "sns.lineplot(x='number of clusters', y='sum of squared errors', data=df)\n",
    "plt.xticks(df['number of clusters'])\n",
    "plt.show()\n",
    "\n",
    "# Plotting silhouette coefficients\n",
    "df = pd.DataFrame({'silhouette coefficient': silhouettes, 'number of clusters': list(range(2, max_k + 1))})\n",
    "sns.lineplot(x='number of clusters', y='silhouette coefficient', data=df)\n",
    "plt.xticks(df['number of clusters'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A projection computed using t-stochastic neighbor embedding (implemented by the class ``TSNE``) can be used to visualise the results of clustering. \n",
    "\n",
    "Recall that dimensionality reduction attemps to represent a dataset by a projection such that each point in the projection corresponds to an observation. The t-stochastic neighbor embedding algorithm attempts to preserve neighbourhoods of the dataset in the corresponding projection.\n",
    "\n",
    "It is important to note that a projection is not, in general, a reliable representation of a dataset. Therefore, care must be taken before deriving conclusions from such visualisations. \n",
    "\n",
    "Note that the numbers assigned to clusters are unlikely to match the numbers assigned to classes. Therefore, only relative colouring is important in the plots shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Computing a projection using t-stochastic neighbour embedding\n",
    "embedding = TSNE(n_components=2, perplexity=50, random_state=0)\n",
    "Xp = embedding.fit_transform(X)\n",
    "\n",
    "# Plotting projection colored by classes\n",
    "df_projection = pd.DataFrame({'x': Xp[:, 0], 'y': Xp[:, 1], 'class': y})\n",
    "sns.scatterplot(x='x', y='y', hue='class', palette=sns.color_palette(), data=df_projection)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=0)\n",
    "y_pred = kmeans.fit_predict(X)\n",
    "\n",
    "# Plotting projection colored by clusters\n",
    "df_projection = pd.DataFrame({'x': Xp[:, 0], 'y': Xp[:, 1], 'cluster': y_pred})\n",
    "sns.scatterplot(x='x', y='y', hue='cluster', palette=sns.color_palette(), data=df_projection)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 [part 2 of 2]\n",
    "\n",
    "For your answers to the assignment, please include include your workings (e.g. equations, code) when this is relevant to the question. Questions 1-5 are pen-and-paper exercises. Questions 6 and 7 below require you to write and provide code.\n",
    "\n",
    "1. Consider a dataset $\\mathcal{D}$ that contains only two observations $\\mathbf{x}_{1} = (1, -1)$ and $\\mathbf{x}_2 = (-1, 1)$. Suppose that the class of the first observation is $y_1 = 0$ and that the class of the second observation is $y_2 = 1$. How would a 1-nearest neighbour classifier based on the Euclidean distance classify the observation $\\mathbf{x} = (-2, 3)$? What are the distances between this new observation and each observation in the dataset? [0.5 marks out of 5]\n",
    "2. Consider a dataset $\\mathcal{D}$ that only contains observations of two different classes. Explain why a $k$-nearest neighbour classifier does not need a tie-breaking policy when $k$ is odd. [0.5 marks out of 5]\n",
    "3. Explain why a classifier that obtains an accuracy of $99.9\\%$ can be terrible for some datasets. [0.5 marks out of 5]\n",
    "4. Consider a classifier tasked with predicting whether an observation belongs to class $y$ (positive class). Suppose that this classifier has precision $1.0$ and recall $0.1$ on a test dataset. If this classifier predicts that an observation does not belong to class $y$, should it be trusted? Should it be trusted if it predicts that the observation belongs to class $y$? [0.5 marks out of 5]\n",
    "5. Based on the confusion matrix shown in this lab notebook, what is the pair of classes that is most confusing for the $1$-nearest neighbour classifier trained in the previous sections? [0.5 marks out of 5]\n",
    "6. Train a support vector machine classifier using the same (subsampled) training dataset used in the previous sections and compute its accuracy on the corresponding test dataset. You can use the default hyperparameters for the class ``SVC`` from ``sklearn.svm``. Show the code in the report. [1.25 marks out of 5]\n",
    "7. Using the same (subsampled) training dataset used in the previous sections, employ ``GridSearchCV`` to find the best hyperparameter settings based on 5-fold cross-validation for a ``RandomForestClassifier``. Consider ``n_estimators`` $ \\in \\{ 50, 100, 200\\}$ and ``max_features`` $ \\in \\{0.1, 0.25\\}$. Use the default values for the remaining hyperparameters. Compute the accuracy of the best model on the corresponding test dataset. Show the code in the report. [1.25 marks out of 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
